{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "haiku_ddpg.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MJX9pF96zz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/deepmind/dm-haiku > /dev/null 2>&1\n",
        "!pip install git+git://github.com/deepmind/rlax.git > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZqNUbPr8h-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import jax\n",
        "from jax import jit, grad, vmap\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental import optix\n",
        "import haiku as hk\n",
        "import rlax\n",
        "\n",
        "import gym\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "from collections import deque\n",
        "from typing import Callable, Mapping, NamedTuple, Tuple, Sequence\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "COLOR = 'white'\n",
        "plt.rcParams['text.color'] = COLOR\n",
        "plt.rcParams['axes.labelcolor'] = COLOR\n",
        "plt.rcParams['xtick.color'] = COLOR\n",
        "plt.rcParams['ytick.color'] = COLORT"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUVyeE848nww",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW21Vg_j8p_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_episodes = 1000\n",
        "max_steps    = 300\n",
        "BATCH_SIZE   = 128\n",
        "GAMMA        = 0.999\n",
        "BUFFER_SIZE  = 1000000\n",
        "NOISE        = 0.1\n",
        "POLYAK       = 0.995\n",
        "SEED         = 1729"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfdOnrLX8sfd",
        "colab_type": "text"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghQupjcX8ud4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot(episode, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title(f'episode {episode}. reward: {np.mean(rewards[-10:])}')\n",
        "    plt.plot(rewards)\n",
        "    plt.show()\n",
        "    # plt.savefig(fname=f\"~/eps_{episode}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH1Q3FZd8yVz",
        "colab_type": "text"
      },
      "source": [
        "# Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tNyAGWk8z3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "    \n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        state      = jnp.expand_dims(state, 0)\n",
        "        next_state = jnp.expand_dims(next_state, 0)\n",
        "            \n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
        "        return {'state': jnp.concatenate(state), 'action':jnp.asarray(action), \n",
        "                'reward':jnp.asarray(reward), \n",
        "                'next_state':jnp.concatenate(next_state), 'done':jnp.asarray(done)}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWH8bmve82kC",
        "colab_type": "text"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8olSugz8143",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jit\n",
        "def scale_action(lower_bound, upper_bound, action):\n",
        "  #(action - min(tanh))/(max(tanh) - min(tanh)) *(max_act - min_act) + min_act\n",
        "  action = lower_bound + (action + 1.0) * 0.5 * (upper_bound - lower_bound) \n",
        "  action = jnp.clip(action, lower_bound, upper_bound)\n",
        "        \n",
        "  return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqhKpCtY87C6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jit\n",
        "def std(a):\n",
        "  return jnp.std(jnp.asarray(a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzcPBTP18_eV",
        "colab_type": "text"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0tOFcMy9BVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jit\n",
        "def polyak_average(old, new):\n",
        "  return jax.tree_multimap(\n",
        "    lambda p_ema, p: p_ema * POLYAK + p * (1. - POLYAK), old, new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJiTJwLh9EW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_actor(num_actions: int) -> hk.Transformed:\n",
        "\n",
        "  def actor(obs):\n",
        "    network = hk.Sequential(\n",
        "        [hk.Linear(400), jax.nn.relu, hk.Linear(300), jax.nn.relu,\n",
        "         hk.Linear(num_actions), jnp.tanh])\n",
        "    return network(obs)\n",
        "\n",
        "  return hk.transform(actor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA2lOt149LJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_critic() -> hk.Transformed:\n",
        "\n",
        "  def q(s, a):\n",
        "    obs = jnp.concatenate((s, a), axis=1)\n",
        "    network = hk.Sequential(\n",
        "        [hk.Linear(400), jax.nn.relu, hk.Linear(300), jax.nn.relu,\n",
        "         hk.Linear(1)])\n",
        "    return network(obs)\n",
        "\n",
        "  return hk.transform(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkThNxSqVcxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main_loop():\n",
        "  # Build env\n",
        "  noise = NOISE\n",
        "  env = gym.make(\"Pendulum-v0\")\n",
        "  replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
        "  rng = hk.PRNGSequence(jax.random.PRNGKey(SEED))\n",
        "  lower_bound   = env.action_space.low\n",
        "  upper_bound = env.action_space.high      \n",
        "  \n",
        "  #logging\n",
        "  rewards = []\n",
        "  returns = []\n",
        "  q = []\n",
        "  g_minus_q = []\n",
        "  episode_reward = 0\n",
        "  ep_idx = 0\n",
        "\n",
        "  # Build and initialize Network.\n",
        "  action_dim = env.action_space.shape[0]\n",
        "  state = env.reset()\n",
        "\n",
        "  actor = build_actor(num_actions)\n",
        "  actor_params = actor.init(next(rng), state)\n",
        "  actor_target = actor_params\n",
        "\n",
        "  action = actor.apply(actor_params, state)\n",
        "  critic = build_critic()\n",
        "  critic_params = critic.init(next(rng), jnp.expand_dims(state, axis=0), \n",
        "                                  jnp.expand_dims(sample_action, axis=0))\n",
        "  critic_target = critic_params\n",
        "\n",
        "  # Build and initialize optimizer.\n",
        "  actor_optimizer = optix.adam(1e-4)\n",
        "  actor_state = actor_optimizer.init(actor_params)\n",
        "\n",
        "  critic_optimizer = optix.adam(1e-3)\n",
        "  critic_state = critic_optimizer.init(critic_params) \n",
        "\n",
        "  @jax.jit\n",
        "  def policy(net_params, key, obs, stddev=1.):\n",
        "    \"\"\"Sample action from epsilon-greedy policy.\"\"\"\n",
        "    action = actor.apply(net_params, obs)\n",
        "    a = rlax.add_gaussian_noise(key, action, stddev)\n",
        "    return a\n",
        "  \n",
        "  batched_dpg_loss = vmap(rlax.dpg_loss)\n",
        "\n",
        "  @vmap \n",
        "  def q_loss(reward, done, q_target, q):\n",
        "    td_target = reward + GAMMA*(1. - done)*q_target\n",
        "    return (jax.lax.stop_gradient(td_target) - q)**2\n",
        "\n",
        "\n",
        "  def actor_step(actor_optimizer, critic_params, actor_target, opt_state, state):\n",
        "    def actor_loss(actor_params, critic_params, state):\n",
        "        a_t = actor.apply(actor_params, state)\n",
        "        dqda_t = grad(critic.apply, argnums=2)(critic_params, state, action)\n",
        "        return jnp.mean(batched_dpg_loss(a_t, dqda_t))\n",
        "    \n",
        "    actor_grad = grad(actor_loss)(actor_params, critic_params, state)\n",
        "    updates, opt_state = actor_optimizer.update(actor_grad, opt_state)\n",
        "    actor_params = optix.apply_updates(actor_params, updates)\n",
        "\n",
        "  def critic_step(critic_optimizer, critic_params, critic_target, actor_target, \n",
        "                  opt_state, batch):\n",
        "    done = batch['done']\n",
        "    state = batch['state']\n",
        "    reward = batch['reward']\n",
        "    action = batch['action']\n",
        "    next_state = batch['next_state']\n",
        "    \n",
        "    def critic_loss(net_params, target_params, actor_target)\n",
        "      state, action, reward, next_state = batch\n",
        "      q = critic.apply(net_params, state, action)\n",
        "      q_target = critic.apply(target_params, next_state, \n",
        "                              actor.apply(actor_target, next_state)) \n",
        "      return jnp.mean(q_loss(reward, done, q_target, q))\n",
        "    \n",
        "    critic_grad = grad(critic_loss)(critic_params, critic_target, \n",
        "                                    actor_target)\n",
        "    updates, opt_state = critic_optimizer.update(critic_grad, opt_state)\n",
        "    critic_params = optix.apply_updates(critic_params, updates)\n",
        "  \n",
        "  @jax.jit\n",
        "  def update(critic_params, actor_params, critic_target, actor_target,\n",
        "             actor_state, critic_state, batch):\n",
        "    \"\"\"Update network weights wrt DPG-learning loss.\"\"\"\n",
        "\n",
        "    loss, dloss_dtheta = jax.value_and_grad(dqn_learning_loss)(net_params, \n",
        "                                                         target_params, batch)\n",
        "    updates, opt_state = optimizer.update(dloss_dtheta, opt_state)\n",
        "    net_params = optix.apply_updates(net_params, updates)\n",
        "    return net_params, opt_state, loss"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}