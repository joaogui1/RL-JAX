{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "haiku_ddpg.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MJX9pF96zz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/deepmind/dm-haiku > /dev/null 2>&1\n",
        "!pip install git+git://github.com/deepmind/rlax.git > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZqNUbPr8h-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import jax\n",
        "from jax import jit, grad, vmap\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental import optix\n",
        "import haiku as hk\n",
        "import rlax\n",
        "\n",
        "import gym\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "from collections import deque\n",
        "from typing import Callable, Mapping, NamedTuple, Tuple, Sequence\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "COLOR = 'white'\n",
        "plt.rcParams['text.color'] = COLOR\n",
        "plt.rcParams['axes.labelcolor'] = COLOR\n",
        "plt.rcParams['xtick.color'] = COLOR\n",
        "plt.rcParams['ytick.color'] = COLORT"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUVyeE848nww",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW21Vg_j8p_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_episodes = 1000\n",
        "max_steps    = 300\n",
        "BATCH_SIZE   = 128\n",
        "GAMMA        = 0.999\n",
        "BUFFER_SIZE  = 1000000\n",
        "NOISE        = 0.1\n",
        "POLYAK       = 0.995\n",
        "SEED         = 1729"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfdOnrLX8sfd",
        "colab_type": "text"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghQupjcX8ud4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot(episode, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title(f'episode {episode}. reward: {np.mean(rewards[-10:])}')\n",
        "    plt.plot(rewards)\n",
        "    plt.show()\n",
        "    # plt.savefig(fname=f\"~/eps_{episode}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH1Q3FZd8yVz",
        "colab_type": "text"
      },
      "source": [
        "# Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tNyAGWk8z3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "    \n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        state      = jnp.expand_dims(state, 0)\n",
        "        next_state = jnp.expand_dims(next_state, 0)\n",
        "            \n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
        "        return {'state': jnp.concatenate(state), 'action':jnp.asarray(action), \n",
        "                'reward':jnp.asarray(reward), \n",
        "                'next_state':jnp.concatenate(next_state), 'done':jnp.asarray(done)}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWH8bmve82kC",
        "colab_type": "text"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8olSugz8143",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jit\n",
        "def scale_action(lower_bound, upper_bound, action):\n",
        "  #(action - min(tanh))/(max(tanh) - min(tanh)) *(max_act - min_act) + min_act\n",
        "  action = lower_bound + (action + 1.0) * 0.5 * (upper_bound - lower_bound) \n",
        "  action = jnp.clip(action, lower_bound, upper_bound)\n",
        "        \n",
        "  return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqhKpCtY87C6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jit\n",
        "def std(a):\n",
        "  return jnp.std(jnp.asarray(a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzcPBTP18_eV",
        "colab_type": "text"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0tOFcMy9BVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jit\n",
        "def polyak_average(old, new):\n",
        "  return jax.tree_multimap(\n",
        "    lambda p_ema, p: p_ema * POLYAK + p * (1. - POLYAK), old, new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJiTJwLh9EW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_actor(num_actions: int) -> hk.Transformed:\n",
        "\n",
        "  def actor(obs):\n",
        "    network = hk.Sequential(\n",
        "        [hk.Linear(400), jax.nn.relu, hk.Linear(300), jax.nn.relu,\n",
        "         hk.Linear(num_actions), jnp.tanh])\n",
        "    return network(obs)\n",
        "\n",
        "  return hk.transform(actor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA2lOt149LJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_critic() -> hk.Transformed:\n",
        "\n",
        "  def q(s, a):\n",
        "    obs = jnp.concatenate((s, a), axis=1)\n",
        "    network = hk.Sequential(\n",
        "        [hk.Linear(400), jax.nn.relu, hk.Linear(300), jax.nn.relu,\n",
        "         hk.Linear(1)])\n",
        "    return network(obs)\n",
        "\n",
        "  return hk.transform(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkThNxSqVcxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main_loop():\n",
        "  # Build env\n",
        "  noise = NOISE\n",
        "  env = gym.make(\"Pendulum-v0\")\n",
        "  replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
        "  rng = hk.PRNGSequence(jax.random.PRNGKey(SEED))\n",
        "  lower_bound   = env.action_space.low\n",
        "  upper_bound = env.action_space.high      \n",
        "  \n",
        "  #logging\n",
        "  rewards = []\n",
        "  returns = []\n",
        "  q = []\n",
        "  g_minus_q = []\n",
        "  episode_reward = 0\n",
        "  ep_idx = 0\n",
        "\n",
        "  # Build and initialize Network.\n",
        "  action_dim = env.action_space.shape[0]\n",
        "  state = env.reset()\n",
        "\n",
        "  actor = build_actor(num_actions)\n",
        "  actor_params = actor.init(next(rng), state)\n",
        "  actor_target = actor_params\n",
        "\n",
        "  action = actor.apply(actor_params, state)\n",
        "  critic = build_critic()\n",
        "  critic_params = critic.init(next(rng), jnp.expand_dims(state, axis=0), \n",
        "                                  jnp.expand_dims(sample_action, axis=0))\n",
        "  critic_target = critic_params\n",
        "\n",
        "  # Build and initialize optimizer.\n",
        "  actor_optimizer = optix.adam(1e-4)\n",
        "  actor_state = actor_optimizer.init(actor_params)\n",
        "\n",
        "  critic_optimizer = optix.adam(1e-3)\n",
        "  critic_state = critic_optimizer.init(critic_params) \n",
        "\n",
        "  @jax.jit\n",
        "  def policy(net_params, key, obs, epsilon):\n",
        "    \"\"\"Sample action from epsilon-greedy policy.\"\"\"\n",
        "    q = network.apply(net_params, obs)\n",
        "    a = rlax.epsilon_greedy(epsilon).sample(key, q)\n",
        "    return a"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}